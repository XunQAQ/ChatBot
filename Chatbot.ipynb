{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3262ed9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\zhaol\\anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhaol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf   \n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import tflearn\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "331638ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the corpus of questions\n",
    "with open(\"questions.json\") as json_corpus: \n",
    "    questions = json.load(json_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8263be42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tag': 'greeting', 'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good day'], 'responses': ['Hello, thanks for visiting', 'Good to see you again', 'Hi there, how can I help?'], 'context_set': ''}\n",
      "{'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye'], 'responses': ['See you later, thanks for visiting', 'Have a nice day', 'Bye! Come back again soon.']}\n",
      "{'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\"], 'responses': ['Happy to help!', 'Any time!', 'My pleasure']}\n",
      "{'tag': 'hours', 'patterns': ['What hours are you open?', 'What are your hours?', 'When are you open?', 'When is the time to contact ?', 'At what time do you provide services ?'], 'responses': [\"We're open every day from 9AM to 9PM\", 'Our working hours are 9AM to 9PM every day']}\n",
      "{'tag': 'location', 'patterns': ['What is your location?', 'Where are you located?', 'What is your address?', 'Where is your company situated?', 'How can we contact you ?', 'How can I contact you?'], 'responses': ['We are on BH-5, LPU', 'Our company is situated in BH-5, LPU', 'We work from BH-5 LPU', 'Our location is BH-5 LPU']}\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 questions\n",
    "for i in range(5):\n",
    "    print(questions[\"questions\"][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f4dcd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists\n",
    "tags = []\n",
    "documents = []\n",
    "words=[]\n",
    "\n",
    "# Starting a loop through each question in questions[\"patterns\"]\n",
    "for question in questions[\"questions\"]:\n",
    "    for pattern in question[\"patterns\"]:\n",
    "        \n",
    "        # Tokenize the question into word by using word tokenizer\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        \n",
    "        # Adding tokenized words to words\n",
    "        words.extend(word) \n",
    "        \n",
    "        # Add words to documents with tag\n",
    "        documents.append((word, question[\"tag\"]))\n",
    "        \n",
    "        # Add non-duplicate tags to tags list\n",
    "        if question[\"tag\"] not in tags:      \n",
    "            tags.append(question[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6dd46595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents:  [(['Hi'], 'greeting'), (['How', 'are', 'you'], 'greeting'), (['Is', 'anyone', 'there', '?'], 'greeting'), (['Hello'], 'greeting'), (['Good', 'day'], 'greeting')]\n",
      "tags:  ['greeting', 'goodbye', 'thanks', 'hours', 'location']\n",
      "words:  ['Hi', 'How', 'are', 'you', 'Is', 'anyone', 'there', '?', 'Hello', 'Good']\n"
     ]
    }
   ],
   "source": [
    "# Display the first 5 documents\n",
    "print(\"documents: \", documents[:5])\n",
    "\n",
    "# Display the the first 5 tags\n",
    "print(\"tags: \", tags[:5])\n",
    "\n",
    "# Display the the first 10 words\n",
    "print(\"words: \", words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53d3ffce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Documents:  106\n",
      "Length of Tags:  37\n",
      "Length of Stemmed Words:  118\n"
     ]
    }
   ],
   "source": [
    "# Lower each word and Perform Stemming by using stemmer\n",
    "stemmer = LancasterStemmer()\n",
    "# Ignori all unwanted punctuation marks.\n",
    "ignore = [\"?\"]\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
    "\n",
    "#Removing Duplicates\n",
    "words = sorted(list(set(words)))\n",
    "tags = sorted(list(set(tags)))\n",
    "\n",
    "#Printing length of lists\n",
    "print(\"Length of Documents: \", len(documents))\n",
    "print(\"Length of Tags: \", len(tags))\n",
    "print(\"Length of Stemmed Words: \", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c55d2f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhaol\\AppData\\Local\\Temp\\ipykernel_26760\\2927712524.py:29: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  train_data = np.array(train_data)\n"
     ]
    }
   ],
   "source": [
    "#Creating Train Data for training\n",
    "train_data = []\n",
    "\n",
    "#Creat empty array for output\n",
    "output_empty = [0 for _ in range(len(tags))]\n",
    "\n",
    "#Create Train set and bag of words for each question\n",
    "for doc in documents:\n",
    "    bag = []\n",
    "    \n",
    "    #Store list of tokenized words for the documents[] in pattern_words\n",
    "    pattern_words = doc[0] \n",
    "    \n",
    "    #Stemme each word in pattern_words\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]  \n",
    "    \n",
    "    #Create bag of words list\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        \n",
    "    #It will give output 1 for curent tag and 0 for all other tags\n",
    "    output_row = list(output_empty)\n",
    "    output_row[tags.index(doc[1])] =1 \n",
    "    train_data.append([bag, output_row])\n",
    "\n",
    "#Creating Training Lists\n",
    "random.shuffle(train_data)\n",
    "#Convert train data into numpy array\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "train_x = list(train_data[:,0])\n",
    "train_y = list(train_data[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "027781e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 13999  | total loss: \u001b[1m\u001b[32m0.00047\u001b[0m\u001b[0m | time: 0.037s\n",
      "| Adam | epoch: 1000 | loss: 0.00047 - acc: 1.0000 -- iter: 104/106\n",
      "Training Step: 14000  | total loss: \u001b[1m\u001b[32m0.00050\u001b[0m\u001b[0m | time: 0.040s\n",
      "| Adam | epoch: 1000 | loss: 0.00050 - acc: 1.0000 -- iter: 106/106\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\zhaol\\JupyterNotebook\\CST8507\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# Clears the default graph stack and resets the global default graph\n",
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "# Build Neural Network layers\n",
    "nn = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "nn = tflearn.fully_connected(nn, 10)\n",
    "nn = tflearn.fully_connected(nn, 10)\n",
    "nn = tflearn.fully_connected(nn, len(train_y[0]), activation=\"softmax\")\n",
    "nn = tflearn.regression(nn)\n",
    "\n",
    "# Define Model\n",
    "model = tflearn.DNN(nn, tensorboard_dir=\"tflearn_logs\") \n",
    "\n",
    "# Train and fit model using train_x, train_y\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "\n",
    "#Save the model\n",
    "model.save(\"model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ee0d176",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Dumping training data by using dump() and writing it into training_data in binary mode\n",
    "pickle.dump({\"words\":words, \"tags\":tags, \"train_x\":train_x, \"train_y\":train_y}, open(\"training_data\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26dadfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all data structure\n",
    "data = pickle.load(open(\"training_data\",\"rb\"))\n",
    "words = data['words']\n",
    "tags = data['tags']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4beac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the corpus of questions\n",
    "with open(\"questions.json\") as json_corpus: \n",
    "    questions = json.load(json_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e05e5a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\zhaol\\JupyterNotebook\\CST8507\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model\n",
    "model.load(\"./model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d9e86a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean User Input\n",
    "def clean_up_sentence(sentence):\n",
    "    \n",
    "    # Tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    #Stemme each word from the user's input\n",
    "    sentence_words= [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c4d2b11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get bag of words array\n",
    "def bow(sentence, words):\n",
    "    \n",
    "    #Tokenize the user input\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    \n",
    "    #Generate bag of words from user input\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s:\n",
    "                bag[i] = 1\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b6ca7f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add some context to the conversation for better results.\n",
    "\n",
    "#Create a dictionary to hold user's context\n",
    "context = {} \n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "\n",
    "def classify(sentence):\n",
    "    \n",
    "    # predict user input and get probabilities using the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    \n",
    "    #Filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    \n",
    "    #Sorting by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    result_list = []\n",
    "    for r in results:\n",
    "        result_list.append((tags[r[0]], r[1]))\n",
    "    \n",
    "    return result_list\n",
    "\n",
    "def response(sentence,userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    \n",
    "    #If we have a classification then find the matching question tag\n",
    "    if results:\n",
    "        # Loop if there are matches to process\n",
    "        while results:\n",
    "            for i in questions['questions']:\n",
    "                \n",
    "                #Find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # print Probability of answer\n",
    "                    if show_details: print ('probability of answer:', results[0][1])\n",
    "                    \n",
    "                    # Set context for this question if necessary\n",
    "                    if 'context_set' in i:\n",
    "                        if show_details: print ('Context:', i['context_set'])\n",
    "                        context[userID] = i['context_set']\n",
    "\n",
    "                    # Check if this question is contextual and applies to this user's conversation\n",
    "                    if not 'context_filter' in i or \\\n",
    "                        (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
    "                        if show_details: print ('Tag:', i['tag'])\n",
    "                        \n",
    "                        # return a random response\n",
    "                        return print('Answer: ', random.choice(i['responses']))\n",
    "            results.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b96fe63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "probability of answer: 0.9997749\n",
      "Tag: black hat hackers\n",
      "Answer:  Black hat hackers are known for having vast knowledge about breaking into computer networks. They can write malware which can be used to gain access to these systems. This type of hackers misuse their skills to steal information or use the hacked system for malicious purpose.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "response(\"what are black hat hackers?\", show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7cc5d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "probability of answer: 0.99985397\n",
      "Tag: encryption\n",
      "Answer:  Encryption is a way of scrambling data so that only authorized parties can understand the information. In technical terms, it is the process of converting plaintext to ciphertext. In simpler terms, encryption takes readable data and alters it so that it appears random. Encryption requires the use of an encryption key: a set of mathematical values that both the sender and the recipient of an encrypted message know.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "response(\"what is encryption?\", show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03c4a381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n",
      "probability of answer: 0.999923\n",
      "Tag: services\n",
      "Answer:  We provide Web Penetration Testing,Android Penetration Testing,Docker Penetration Testing,Vulnerability Assessment,Cyber Crime investigation and many more services.\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "response(\"what can you help me?\", show_details=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28021b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
